# NoML--No Machine Learning

No matter how you end up reading this post, if you will interview machine learning engineer or data scientist job,
you must be well prepared for that. Do you know machine learning? How well?  Consider the following questions:

```
*  What's bias and variance?
*  What's L1 and L2 regularization? which is more stable?
*  What's relationship between logistic regression and SVM?
*  What's SVM? please derive it mathematically.
*  How to update logistic regression parameters? which algorithms? 
*  Do you know deep learning? please write a basic fully-connected neural network with numpy using any activation function?

```
If you could answer those questions pretty well(Chinese "pretty well", not....), I think you're ready for the interview. 
Machine learning interview questions can be asked as simple as explaining confusion matrix, drawing ROC, or as hard as implementing K-means, basic neural networks or even some mathematical explanation of SVM. These are really basic, not even considering practical issue. 

You might think that we can read a ML book, why do we need to create this notebook? Because sometimes, when we read a book, we get lost and  don't know what kind of question will be asked.

So the beauty of this book is to **collect questions or asked questions!**

## Structure

For each topic, like overfitting, we simply ask questions, and each answer should be completed within_**140 words**_. Yes, I do believe that Twitter gives us a golden rule, anything should be explained within 140 words. If within 140 words,  you can't give the main points of this question, then basically mean you don't know this problem.

Then if necessary, you can give an explanation. Please note,** you can asked any questions you want.  But the answer you provided should always come with a link\(give credits to the original author\).  **At least, let the readers judge how good the answer is and whether it makes sense!

For example,

```markdown
Q: What's overfitting?

A: Overfitting refers to a model that models the training data too well.
   Overfitting happens when a model learns the detail and noise in 
   the training data to the extent that it negatively impacts the 
   performance of the model on new data.

E: You can give some explanations here.

R: https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/
```

## Read at GitBook

This project is hosted at gitbook, you can read it here!==&gt;[https://weifoo.gitbooks.io/noml/content](https://weifoo.gitbooks.io/noml/content/)



